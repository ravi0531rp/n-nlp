{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain components to use\n",
    "from langchain.vectorstores.cassandra import Cassandra\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Support for dataset retrieval with Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "# With CassIO, the engine powering the Astra DB integration in LangChain,\n",
    "# you will also initialize the DB connection:\n",
    "import cassio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASTRA_DB_APPLICATION_TOKEN = \"AstraCS:YkslDvqyPTWOeoiXYHTPjTlP:75b14ed0fbadd693bb0c77c77aab969a7b087faa1f55498ee118841c7d40001e\" \n",
    "ASTRA_DB_ID = \"99498bcd-ef25-4498-b948-f5fccf9ed132\"\n",
    "\n",
    "OPENAI_API_KEY = \"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfreader = PdfReader('/home/ravi0531rp/Downloads/aaa.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Concatenate\n",
    "# read text from pdf\n",
    "raw_text = ''\n",
    "for i, page in enumerate(pdfreader.pages):\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        raw_text += content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Real-Time Flying Object Detection with YOLOv8\\nDillon Reis*, Jordan Kupec, Jacqueline Hong, Ahmad Daoudi\\nGeorgia Institute of Technology\\ndreis7@gatech.edu *, jkupec3@gatech.edu, jhong356@gatech.edu, adaoudi3@gatech.edu\\nAbstract\\nThis paper presents a generalized model for real-time\\ndetection of ﬂying objects that can be used for transfer\\nlearning and further research, as well as a reﬁned model\\nthat is ready for implementation. We achieve this by train-\\ning our ﬁrst (generalized) model on a data set containing\\n40 different classes of ﬂying objects, forcing the model to\\nextract abstract feature representations. We then perform\\ntransfer learning with these learned parameters on a data\\nset more representative of “real world” environments (i.e.\\nhigher frequency of occlusion, small spatial sizes, rotations,\\netc.) to generate our reﬁned model. Object detection of ﬂy-\\ning objects remains challenging due to large variance ob-\\nject spatial sizes/aspect ratios, rate of speed, occlusion, and\\nclustered backgrounds. To address some of the presented\\nchallenges while simultaneously maximizing performance,\\nwe utilize the current state of the art single-shot detector,\\nYOLOv8, in an attempt to ﬁnd the best trade-off between\\ninference speed and mAP . While YOLOv8 is being regarded\\nas the new state-of-the-art [16], an ofﬁcial paper has not\\nbeen provided. Thus, we provide an in depth explanation\\nof the new architecture and functionality that YOLOv8 has\\nadapted. Our ﬁnal generalized model achieves an mAP50-\\n95 of 0.685 and average inference speed on 1080p videos\\nof 50 fps. Our ﬁnal reﬁned model maintains this inference\\nspeed and achieves an improved mAP50-95 of 0.835.\\n1. Introduction/Background/Motivation\\nNumerous recent events have demonstrated the mali-\\ncious use of drones. Over the past few months, there have\\nbeen reports of assassination attempts via drones with small\\nexplosive payloads [17], drug deliveries to state prisons\\n[19], and surveillance of U.S. Border Patrol by smugglers\\n[5] to exploit weaknesses. While research indicates that\\ndrone usage is expected to increase exponentially [15], de-\\ntection technology has yet to provide reliable and accurate\\nresults. Drones and mini UA Vs present a stealth capabil-\\nity and can avoid detection by most modern radar systems\\ndue to their small electromagnetic signature. They are alsosmall, highly maneuverable, and omit low levels of noise.\\nThis, along with the ease of access provides a natural in-\\ncentive for drones to remain an integral part of modern war-\\nfare and illegal activities. While methods such as radio and\\nacoustic detection have been proposed as solutions, they are\\ncurrently known to be inaccurate [4]. This motivates the in-\\ntegration of a visual detector in any such detection system.\\nThe U.S. Border Patrol implements real-time object detec-\\ntion from digital towers to monitor people and motor ve-\\nhicles [2], but are not currently known to implement drone\\ndetection. Drone detection in this environment is challeng-\\ning due to the cluttered desert background and distance that\\ndrones survey from. [6]. The farther the drone is from cam-\\neras, the more difﬁcult it will be to detect and classify it, as\\nthe object will convey less signal in the input space to the\\nmodel.\\nOur primary objective is to provide a generalized real-\\ntime ﬂying object detection model that can be used by oth-\\ners for transfer learning or further research, as well as a re-\\nﬁned model that is ready to use “out of the box” for imple-\\nmentation [14]. We deﬁne a generalized model as one that\\nhas good detection and classiﬁcation performance on a large\\nnumber of classes at higher resolutions while maintaining a\\nreasonable frame rate (1080p : 30-60 frames per second).\\nInstead of just training our model on drones, we train on a\\ndata set containing 40 ﬂying object categories to force the\\nmodel to learn more abstract feature representations of ﬂy-\\ning objects. Then, we transfer learn these weights on a ﬁ-\\nnal data set containing more instances of “real world“ envi-\\nronments (i.e. higher frequency of occlusion, small spatial\\nsizes, rotations, etc.). This in turn will lead to a more re-\\nﬁned, ready to implement real-time ﬂying object detection\\nmodel. To maximize our model’s performance, we use the\\nlatest state-of-the-art single-shot detector, YOLOv8. Cur-\\nrently, single-stage detectors are the de-facto architecture\\nchoice for fast inference speeds. This choice comes at the\\nexpense of exchanging the higher accuracy you would typ-\\nically expect from a two-state detector. While YOLOv8 is\\nbeing regarded as the new state-of-the-art [16], an ofﬁcial\\npaper has yet to be released. This motivates our secondary\\nobjective, which is to explain the new architecture and func-\\n1arXiv:2305.09972v1  [cs.CV]  17 May 2023tionality that YOLOv8 has adapted.\\nReal-time object detection remains challenging due to\\nvariances in object spatial sizes and aspect ratios, inference\\nspeed, and noise. This is especially true for our use case, as\\nﬂying objects can change location, scale, rotation, and tra-\\njectory very quickly. This conveys the necessity for fast in-\\nference speed and thorough model evaluation between low-\\nvariance classes, object sizes, rotations, backgrounds, and\\naspect ratios.\\nOur initial model is trained on a data set [11] com-\\nprised of 15,064 images of various ﬂying objects with an\\n80% train and 20% validation split. Each image is la-\\nbeled with the class number of the object and the coordi-\\nnates of the edges of the associated bounding box. An im-\\nage may have more than one object and class, sitting at an\\naverage of 1.6 annotated objects per image and a total of\\n24,769 annotations across all images. The median image\\nratio is 416x416. The images were pre-processed with auto-\\norientation, but there were no augmentations applied. The\\ndata set represents a long-tailed distribution with the drone\\n(25.2% of objects), bird (25%), p-airplane (7.9%), and c-\\nhelicopter (6.3%) classes taking up the majority of the data\\nset (64.4%), suffering from a class imbalance. Published on\\nRoboﬂow with an unnamed author, this data set was gener-\\nated in 2022, having been downloaded only 15 times.\\nIn addition, we utilized a second data set [1] to apply\\ntransfer learning for the reﬁned model. With a focus on\\nthe challenges we laid out, this second data set consists of\\nﬂying objects at a noticeably farther distance than our ini-\\ntial data set. It consists of 11,998 images, where the av-\\nerage image size is 0.33 mp with a median image ratio of\\n640x512. The images are separated into a 90% train and\\n10% validation split. An image may contain more than one\\nobject and class, however, it has an average of one object\\nper image, reaching a total count of 12,410 annotated ob-\\njects. With only four different objects, each class is well\\nrepresented: drones take up 38.8% of the annotated objects,\\n21.2% helicopters, 20.4% airplanes, and 19.6% birds. Al-\\nthough Roboﬂow reports a bird class, the images that con-\\ntain birds are not labeled and are not included as a class in\\nthe transfer model. This dataset was published on Roboﬂow\\nin 2022 by Ahmed Mohsen [1], having only 5 downloads\\nby the time of this paper.\\n2. Approach\\nWe chose the YOLOv8 architecture under the assump-\\ntion that it would provide us the highest probability of suc-\\ncess given the task. YOLOv8 is assumed to be the new\\nstate-of-the-art due to its higher mAPs and lower inference\\nspeed on the COCO dataset. However, an ofﬁcial paper\\nhas yet to be released. It also speciﬁcally performs bet-\\nter at detecting aerial objects 7b. We implement the code\\nfrom the Ultralytics repository. We decide to implementtransfer learning and initialize our models with pre-trained\\nweights to then begin training on the custom data set. These\\nweights are from a model trained on the COCO dataset. Due\\nto only having access to a single NVIDIA RTX 3080 and\\n3070, a greedy model selection/hyper-parameter tuning ap-\\nproach was chosen. We ﬁrst train a version of the small,\\nmedium, and large versions of the model with default hyper-\\nparameters for 100 epochs. Then, we decide which model\\nis optimal for our use case given the trade off between infer-\\nence speed and mAP-50-95 on the validation set. After the\\nmodel size is selected, a greedy hyper-parameter search is\\nconducted with 10 epochs per each set of hyper-parameters.\\nThe model with the optimal hyper-parameters trains for 163\\nepochs to generate the generalized model. After this model\\nlearns abstract feature representations for a wide-array of\\nﬂying objects, we then transfer learn these weights to a data\\nset that is more representative of the real world [1] to gen-\\nerate the reﬁned model. This data set contains 3 classes:\\nhelicopter, plane, and drone, with very high variance in ob-\\nject spatial sizes. For evaluation, we are particularly inter-\\nested in evaluating mAP50-95 and inference speed, as these\\nare the most common measures of success across most ob-\\nject detection algorithms. Due to the large class imbalance,\\npoor performance on the validation set was anticipated in\\nthe minority classes. However, this was not observed 1a.\\nMean average precision (mAP) is one of the most used\\nevaluation metrics for object detection. mAP takes the av-\\nerage precision (AP) over all classes and computes them\\nat a pre-speciﬁed IoU threshold. To deﬁne precision we\\nneed to deﬁne true positives and false positives for object\\ndetection. A true positive will be determined when the IoU\\nbetween the predicted box and ground truth is greater than\\nthe set IoU threshold, while a false positive will have the\\nIoU below that threshold. Then, precision can be deﬁned as\\ntp\\ntp+fp. We take the mean over a class by iterating over\\na set of thresholds and averaging them. For mAP50-95, we\\ntake steps of 0.05 starting from an IoU threshold of 0.5 and\\nstopping at 0.95. The average precision over this interval\\nis the class AP. Do this for all classes and take the average\\nover them and we generate the mAP50-95.\\n2.1. Model Choice and Evaluation\\nWe evaluate small, medium, and large versions of the\\nmodels to determine an optimal trade off between infer-\\nence speed and mAP50-95 to then optimize the hyper-\\nparameters. The small, medium, and large models\\nhave (11151080, 25879480, & 43660680) parameters and\\n(225,295, & 365) layers respectively. After training the\\nmodels we see there is a noticeable increase in mAP50-95\\nbetween small and medium models (0.05), but not much\\ndelta between medium and large (0.002). We also see that\\nsmall, medium, and large infer at 4.1, 5.7, and 9.3 millisec-\\n2onds respectively on the validation set. However, our orig-\\ninal goal is to reach an average inference speed between\\n30 to 60 frames for 1080p. When testing the medium size\\nmodel on multiple 1080p HD videos, we observe an av-\\nerage total speed (pre-proccess speed(0.5ms) + inference\\nspeed(17.25ms) + post-process speed(2ms)) of 19.75 ms\\n(50 frames per second), which aligns with our primary ob-\\njective. This leads to our selection of the medium size\\nmodel to begin tuning hyper-parameters.\\nDue to a lack of computational resources, we evaluate 10\\nepochs for each set of hyper-parameters as an indicator for\\nthe potential performance of additional epochs. We observe\\nthat this assumption is correct, as training with the opti-\\nmal set of hyper-parameters achieves better performance at\\nepoch 100 compared to default hyper-parameters (0.027)1b\\nWe choose the best hyper-parameters based on validation\\nmAP50-95 as batch size of 16, stochastic gradient descent\\n(SGD) as the optimizer, momentum of 0.937, weight decay\\nof 0.01, classiﬁcation loss weight \\x15cls= 1, box loss weight\\n\\x15box= 5.5, and distribution focal loss weight \\x15dfl= 2.5.\\nAfter training for 163 epochs we achieve an mAP50-95 of\\n0.685 and an average inference speed on 1080p videos of\\n50 fps.\\n2.2. Loss Function and Update Rule\\nThe generalized loss function and weight update proce-\\ndure can be deﬁned as follows:\\nL(\\x12) =\\x15box\\nNposLbox(\\x12)+\\x15cls\\nNposLcls(\\x12)+\\x15dfl\\nNposLdfl(\\x12)+\\x1ek\\x12k2\\n2\\n(1)\\nVt=\\x0cVt\\x001+r\\x12L(\\x12t\\x001) (2)\\n\\x12t=\\x12t\\x001\\x00\\x11Vt(3)\\nWhere 1 is the generalized loss function incorporating\\nthe individual loss weights and a regularization term with\\nweight decay \\x1e, 2 is the velocity term with momentum \\x0c,\\nand 3 which is the weight update rule and \\x11is the learning\\nrate. The speciﬁc YOLOv8 loss function can be deﬁned as:\\nL=\\x15box\\nNposX\\nx;y1c\\x03x;y\\x02\\n1\\x00qx;y+kbx;y\\x00^bx;yk2\\n2\\n\\x1a2+\\x0bx;y\\x17x;y\\x03\\n+\\x15cls\\nNposX\\nx;yX\\nc2classesyclog(^yc) + (1\\x00yc)log(1\\x00^yc)\\n+\\x15dfl\\nNposX\\nx;y1c\\x03x;yh\\n\\x00(q(x;y)+1\\x00qx;y)log(^qx;y)\\n+ (qx;y\\x00q(x;y)\\x001)log(^q(x;y)+1)i\\n(4)\\nwhere:\\nqx;y=IoUx;y=^\\x0cx;y\\\\\\x0cx;y\\n^\\x0cx;y[\\x0cx;y\\x17x;y=4\\n\\x192(arctan (wx;y\\nhx;y)\\x00arctan (^wx;y\\n^hx;y))2\\n\\x0bx;y=\\x17\\n1\\x00qx;y\\n^yc=\\x1b(\\x01)\\n^qx;y=softmax (\\x01)\\nand:\\n•Nposis the total number of cells containing an object.\\n• 1c\\x03x;yis an indicator function for the cells containing\\nan object.\\n•\\x0cx;yis a tuple that represents the ground truth bound-\\ning box consisting of ( xcoord ,ycoord , width, height).\\n•^\\x0cx;yis the respective cell’s predicted box.\\n•bx;yis a tuple that represents the central point of the\\nground truth bounding box.\\n•ycis the ground truth label for class c (not grid cell c)\\nfor each individual grid cell (x,y) in the input, regard-\\nless if an object is present.\\n•q(x;y)+=\\x001are the nearest predicted boxes IoUs (left\\nand right)2c\\x03\\nx;y\\n•wx;yandhx;yare the respective boxes width and\\nheight.\\n•\\x1ais the diagonal length of the smallest enclosing box\\ncovering the predicted and ground truth boxes.\\nEach cell then determines its best candidate for predict-\\ning the bounding box of the object. This loss function in-\\ncludes the CIoU (complete IoU) loss proposed by Zheng et\\nal.[22] as the box loss, the standard binary cross entropy for\\nmulti-label classiﬁcation as the classiﬁcation loss (allowing\\neach cell to predict more than 1 class), and the distribution\\nfocal loss proposed by Li et al.[10] as the 3rd term.\\n3. Experiments and Results\\n3.1. Model Confusion and Diagnosis\\nOne of the primary challenges in object detection is deal-\\ning with data sets with low inter-class variance, i.e., multi-\\nple classes that look similar to each other compared to the\\nrest of the labels. Take, for example, the F-14 and F-18.\\nBoth have similar-looking wing shapes, two rudders, an en-\\ngine, a cockpit, and a respective payload. In this confusion\\nmatrix 1, the model is most likely to mis-classify an F-14\\nas an F-18. This type of mis-classiﬁcation typically affects\\nclasses in categories with low inter-class variance amongst\\nthemselves. Visualizing activation maps [21] is a technique\\n3(a) Confusion matrix for all classes\\n (b) YOLOv8 validation mAP50-95\\nFigure 1: YOLOv8 Validation\\nFigure 2: Feature Activation maps for the F-14 and F-18 ﬁghter jets. From left to right we have the four stages of the model’s\\nCSPDarkNet53 backbone\\nthat helps us understand what pixels in the input image are\\nimportant for determining its class.\\nGenerally, deeper layers in CNNs extract more granu-\\nlar/complex/low level feature representations. YOLOv8 in-\\ncorporates this idea into its architecture by having repeating\\nmodules and multiple detection heads when making its pre-\\ndiction. For our experimentation, we use MMYolo [21] to\\ncreate activation maps at different stages of our backbone.\\nWe expect to some sense of differentiation in the different\\nfeature maps. If our model shows similar feature activations\\nfor F-14s and F-18s, we can say that may be the reason for\\nclass confusion.\\nMMYolo [21] by Yamaguchi et al. is an open-sourcetoolbox for YOLO series algorithms based on PYTorch.\\nMMYolo can decompose the most popular YOLO algo-\\nrithms, making them easily customizable and ready for\\nanalysis. For our analysis, we employed MMYolo to ﬁrst\\nconvert the weights from .pt (Pytorch model) to .pth (State\\ndictionary ﬁle, i.e., weights, bias, etc.) and second visualize\\nthe different activation maps of YOLOv8 during inference.\\nMMYolo allows you to specify model type, weight ﬁle, tar-\\nget layer, and channel reduction.\\nYOLOv8 6 uses CSPDarknet53 [13] as its backbone, a\\ndeep neural network that extracts features at multiple res-\\nolutions (scales) by progressively down-sampling the in-\\nput image. The feature maps produced at different reso-\\n4Figure 3: (a) picks up the drone, (b) picks up tree top granularity - tree tops are more granular than stumps, (c) granular\\nversion of layer (b), (d) an outlier, texturized analysis of what the object is.\\nlutions contain information about objects at different scales\\nin the image and different levels of detail and abstraction.\\nYOLOv8 can incorporate different feature maps at different\\nscales to learn about object shapes and textures, which helps\\nit achieve high accuracy in most object detection tasks.\\nYOLOv8 backbone consists of four sections, each with a\\nsingle convolution followed by a c2f module [16]. The c2f\\nmodule is a new introduction to CSPDarknet53. The mod-\\nule comprises splits where one end goes through a bottle-\\nneck module(Two 3x3 convolutions with residual connec-\\ntions. The bottleneck module output is further split N times\\nwhere N corresponds to the YOLOv8 model size. These\\nsplits are all ﬁnally concatenated and passed through one ﬁ-\\nnal convolution layer. This ﬁnal layer is the layer we will\\nget the activations.\\nThis ﬁgure 2 shows the original F-14 and F-18 images\\nand the activations of the four c2f stages in the network,\\nwith each stage being more profound in the network from\\nthe second image right. The Activation Map corresponding\\nto the shallowest c2f module shows the broadest activation.\\nThis module detects the two wings of the aircraft and de-\\ntermines that this object is a plane. The second activation\\nmap corresponds to the second c2f module in our backbone.\\nIt shows strong activations at different components of the\\naircraft, such as locating the wings, body, cockpit, and pay-\\nload. It appears that this layer is attempting to infer what\\nkind of aircraft is being presented in the image by high-\\nlighting these features. The third activation map is starting\\nto dive into the individual textures of the components of the\\naircraft, presumably checking for minute differences in the\\njets structure. Finally, the model’s ﬁnal c2f module activates\\nextremely ﬁne-grained details and outlines in the respectiveimages. These similar feature activation maps could be the\\nreason that the model confuses the two.\\n3.2. Model Examples and Results\\nTo highlight our results, we address three challenging\\nconditions: (1) detecting and classifying extremely small\\nobjects, (2) identifying ﬂying objects that blend into their\\nbackground, and (3) classifying different types of ﬂying\\nobjects. We examined the performance of our generalized\\nmodel,[11], against these challenges. This is demonstrated\\nin Figure 4a, which features four images that represent the\\nbird, drone, passenger airplane, and V22 classes.\\nThe ﬁrst of the four images showcases the model’s abil-\\nity to identify distant birds. In the second image, the model\\nwas put to the test against a very small drone that occupied\\nonly .026% of the image size while also blending in with its\\nbackground. The model still resulted in the correct detec-\\ntion and classiﬁcation of the drone. The third image shows\\nthe model’s ability to identify a minute passenger airplane\\nof size 0.063% of the image, which is also blended into its\\nsurroundings. Finally, the fourth image features a V22 air-\\ncraft, which is an underrepresented class and accounts for\\nonly 3.57% of the entire dataset. A V22 can easily be mis-\\ntaken as a drone due to its vertical propeller positioning.\\nDespite these two characteristics and only taking up 0.14%\\nof the entire image, the image exhibits the model’s ability to\\nstill identify the V22 with impressive accuracy, achieving a\\nconﬁdence score of 0.83.\\nDespite the visual similarities between the birds, drones,\\nand passenger airplanes in these images, our model success-\\nfully classiﬁed them with adequate conﬁdence. These re-\\nsults illustrate our model’s ability to overcome our identiﬁed\\n5(a) Generalized Model Images\\n(b) Reﬁned Model - Transfer Learning Images\\nFigure 4: Prediction Images\\nchallenges associated with object detection in real-world\\nconditions, and also demonstrate our success in creating a\\nsolution that effectively tackles these challenges. Overall,\\nit does very well at distinguishing various types of ﬂying\\nobjects despite the need to account for multiple different\\nclasses of aircraft.\\n3.3. Reﬁned Model - Transfer Learning\\nAfter applying transfer learning to our “real world“ data\\nset [1], the results indicate that our model is a solid foun-\\ndation for transfer learning and effectively extracted ﬂying\\nobject feature representations. This data set was selected\\nto focus on our challenge of detecting and classifying ex-\\ntremely small objects in appearance. Figure 4b displays our\\nresults, featuring four distinct images that represent the bird,\\ndrone, airplane, and helicopter objects.\\nThe ﬁrst image displays a small, pixelated bird that only\\ntakes up 0.02% of the image. Even with the lack of the bird\\nclass in our training process, our model correctly identiﬁed\\nthat the object was not any of the other available classes,\\neven while allowing a very low conﬁdence threshold of\\n0.20. The second image contains a drone, which also only\\ntook up 0.02% of its image. This drone is nearly indistin-\\nguishable from the background clouds to the human eye,\\nyet our model was still able to classify it with a conﬁdence\\nscore of 0.81. The third image includes a small airplane\\nthat takes up 0.034% of pixels, which our model was still\\nable to correctly identify and classify with a high conﬁdencescore of 0.85. In the ﬁnal image, a barely visible helicopter\\n(0.01% of the image) was correctly classiﬁed with a conﬁ-\\ndence score of 0.73.\\nFigure 3 we can see that in the ﬁrst layer, the feature\\nmap activation correctly segments the object. The second\\nlayer starts picking out all of the tree tops which can be\\nexplained by the higher relative variance of the tree tops. In\\nthe third layer, we see mroe importance being placed on the\\nbackground and more granular features being detected. In\\nthe fourth layer, we see the outline of the drone itself.\\nMoving on to the second row, we can see the\\ntrue strength of the localization accuracy with an over-\\nemphasized detection. In the second layer, we see a de-\\nemphasis on the background. In the third and fourth layer,\\nwe see the same behavior as before.\\nOur model performed exceptionally well, even while\\npresented with the challenges of size, varying ﬂying ob-\\njects, and camouﬂaged objects that would be difﬁcult for\\nthe human eye to identify. These results demonstrate that\\nour model serves as an excellent base for transfer learn-\\ning, particularly when dealing with small, pixelated objects,\\nblended backgrounds, and distinguishing between drones\\nand other ﬂying objects. After 190 epochs with the weights\\nlearned from the generalized model as the initialization, the\\nreﬁned model achieves an mAP50-95 of 0.835 across the\\nplane, helicopter, and drone classes.\\n64. Model Architecture\\nWith the publication of “You Only Look Once: Uni-\\nﬁed, Real-Time Object Detection” ﬁrst proposed by Red-\\nmon et al.[12] in 2015, one of the most popular object de-\\ntection algorithms, YOLOv1, was ﬁrst described as hav-\\ning a “refreshingly simple” approach [18]. At its incep-\\ntion, YOLOv1 could process images at 45 fps, while a vari-\\nant, fast YOLO, could reach upwards of 155 fps. It also\\nachieved high mAP compared to other object detection al-\\ngorithms at the time.\\nThe main proposal from YOLO is to frame object detec-\\ntion as a one-pass regression problem. YOLOv1 comprises\\na single neural network, predicting bounding boxes and as-\\nsociated class probability in a single evaluation. The base\\nmodel of YOLO works by ﬁrst dividing the input image into\\nan S x S grid where each grid cell (i,j) predicts B bounding\\nboxes, a conﬁdence score for each box and C class proba-\\nbilities. The ﬁnal output will be a tensor of shape: S x S x\\n(B x 5 + C).\\n4.1. YOLOv1 Overview\\nYOLOv1 architecture 5 consists of 24 convolutional lay-\\ners followed by two fully connected layers. In the pa-\\nper, the authors took the ﬁrst 20 convolutional layers from\\nthe backbone of the network and, with the addition of an\\naverage pooling layer and a single fully connected layer,\\nwhere it was pre-trained and validated on the ImageNet\\n2012 dataset. During inference, the ﬁnal four layers and 2\\nFC layers are added to the network; all initialized randomly.\\nFigure 5: YOLO Architecture [12]\\nYOLOv1 uses stochastic gradient descent as its opti-\\nmizer; the Loss function is shown here 5. The Loss function\\n5 comprises two parts, localization loss, and classiﬁcation\\nloss. The localization loss measures the error between the\\npredicted bounding box coordinates and the ground-truth\\nbounding box. The classiﬁcation loss measures the error be-\\ntween the predicted class probabilities and the ground truth.\\nThe\\x15coord and\\x15noobj are regularization coefﬁcients that\\nregulate the magnitude of the different components, em-\\nphasizing object localization and deemphasizing grid cells\\nwithout objects.\\x15coordS2X\\ni=0BX\\nj=01obj\\nijh\\n(xi\\x00^xi)2+ (yi\\x00^yi)2i\\n+\\x15coordS2X\\ni=0BX\\nj=01obj\\nijh\\n(pwi\\x00p\\n^wi)2+ (p\\nhi\\x00q\\n^hi)2i\\n+S2X\\ni=0BX\\nj=01obj\\nij(Ci\\x00^Ci)2\\n+\\x15noobjS2X\\ni=0BX\\nj=01noobj\\nij(Ci\\x00^Ci)2\\n+S2X\\ni=01obj\\niX\\nc2classes(pi(c)\\x00^pi(c))2(5)\\n4.2. YOLOv5 Overview\\nYOLOv5 [4] is an object detection model introduced in\\n2020 by Ultralytics, the originators of the original YOLOv1\\nand YOLOv3. YOLOv5 achieves SOTA performance on\\nthe COCO benchmark dataset [3] while also being fast and\\nefﬁcient to train and deploy. YOLOv5 made several archi-\\ntectural changes, most notably the standardized practice of\\nstructuring the model into three components, the backbone,\\nneck, and head.\\nThe backbone of YOLOv5 is Darknet53, a new network\\narchitecture that focuses on feature extraction characterized\\nby small ﬁlter windows and residual connections. Cross\\nStage Partial connections enable the architecture to achieve\\na richer gradient ﬂow while reducing computation as de-\\nscribed [20] proposed by Wang et al.\\nThe neck [18], as described by Teven et al., of YOLOv5\\nconnects the backbone to the head, whose purpose is to ag-\\ngregate and reﬁne the features extracted by the backbone,\\nfocusing on enhancing the spatial and semantic information\\nacross different scales. A Spatial Pyramid Pooling (SPP)\\n[8] module removes the ﬁxed-size constraint of the net-\\nwork, which removes the need to warp, augment, or crop\\nimages. This is followed by a CSP-Path Aggregation Net-\\nwork [20] module, which incorporates the features learned\\nin the backbone and shortens the information path between\\nlower and higher layers.\\nYOLOv5’s head consists of three branches, each predict-\\ning a different feature scale. In the original publication of\\nthe model [3], the creators used three grid cell sizes of 13 x\\n13, 26 x 26, and 52 x 52, which each grid cell predicting B\\n= 3 bounding boxes. Each head produces bounding boxes,\\nclass probabilities, and conﬁdence scores. Finally, the net-\\nwork uses Non-maximum Suppression (NMS) [9] to ﬁlter\\nout overlapping bounding boxes.\\n7Figure 6: YOLOv8 Architecture [16]\\nYOLOv5 incorporates anchor boxes, ﬁxed-sized bound-\\ning boxes used to predict the location and size of objects\\nwithin an image. Instead of predicting arbitrary bounding\\nboxes for each object instance, the model predicts the co-\\nordinates of the anchor boxes with predeﬁned aspect ratios\\nand scales and adjusts them to ﬁt the object instance.\\n4.3. YOLOv8 Overview\\nYOLOv8 is the latest version of the YOLO object detec-\\ntion model. This latest version has the same architecture as\\nits predecessors 6 but it introduces numerous improvements\\ncompared to the earlier versions of YOLO such as a new\\nneural network architecture that utilizes both Feature Pyra-\\nmid Network (FPN) and Path Aggregation Network (PAN)\\nand a new labeling tool that simpliﬁes the annotation pro-\\ncess. This labeling tool contains several useful features like\\nauto labeling, labeling shortcuts, and customizable hotkeys.\\nThe combination of these features makes it easier to anno-\\ntate images for training the model.\\nThe FPN works by gradually reducing the spatial res-\\nolution of the input image while increasing the number\\nof feature channels. This results in the creation of fea-\\nture maps that are capable of detecting objects at different\\nscales and resolutions. The PAN architecture, on the other\\nhand, aggregates features from different levels of the net-\\nwork through skip connections. By doing so, the network\\ncan better capture features at multiple scales and resolu-\\ntions, which is crucial for accurately detecting objects of\\ndifferent sizes and shapes. [18]\\n4.4. YOLOv8 vs YOLOv5\\nThe reason YOLOv8 is being compared to YOLOv5\\nand not any other version of YOLO is that YOLOv5’s\\nperformance and metrics are closer to YOLOv8’s. How-\\never, YOLOv8 surpasses YOLOv5 in aspects including\\na better mAP as seen in Figure 7a. Along with a bet-\\nter mAP, this shows that YOLOv8 has fewer outliers\\nwhen measured against the RF100 which is a 100-sample\\ndataset from the Roboﬂow universe which is a repository\\nof 100,000 datasets. We also witness YOLOv8 outperform-ing YOLOv5 for each RF100 category. From Figure 7b we\\ncan see that YOLOv8 produces similar or even better results\\ncompared to YOLOv5 [16].\\nAs mentioned previously, YOLOv8 uses a new architec-\\nture that combines both FAN and PAN modules. FPN is\\nused to generate feature maps at multiple scales and reso-\\nlutions, while PAN is used to aggregate features from dif-\\nferent levels of the network to improve accuracy. The re-\\nsults of the combined FAN and PAN modules are better\\nthan YOLOv5 which uses a modiﬁed version of CSPDark-\\nnet architecture. This modiﬁed version of CSPDarknet is\\nbased on the cross-stage partial connections (CSP), which\\nimproves the ﬂow of information between different parts of\\nthe network.\\n(a) YOLOs mAP@.50\\nagainst RF100 [16]\\n(b) YOLOs average\\nmAP@.50 against\\nRF100 categories\\nFigure 7: YOLOv8 vs Previous Versions\\nAnother difference the two models have is their training\\ndata. YOLOv8 was trained on a larger and more diverse\\ndataset compared to YOLOv5. YOLOv8 was trained on a\\nblend of the COCO dataset and several other datasets, while\\nYOLOv5 was trained primarily on the COCO dataset. Be-\\ncause of that, YOLOv8 has a better performance on a wider\\nrange of images.\\nYOLOv8 includes a new labeling tool called RoboFlow\\nAnnotate which is used for image annotation and object\\ndetection tasks in computer vision. RoboFlow Annotate\\nmakes it easier to annotate images for training the model\\nand includes several features such as auto labeling, labeling\\nshortcuts, and customizable hotkeys. In contrast, YOLOv5\\nuses a different labeling tool called LabelImg. LabelImg is\\n8an open-source graphical image annotation tool that allows\\nits users to draw bounding boxes around objects of interest\\nin an image, and then export the annotations in the YOLO\\nformat for training the model.\\nYOLOv8 includes more advanced post-processing tech-\\nniques than YOLOv5, which is a set of algorithms ap-\\nplied to the predicted bounding boxes and objectiveness\\nscores generated by the neural network. These techniques\\nhelp to reﬁne the detection results, remove redundant de-\\ntections, and improve the overall accuracy of the predic-\\ntions. YOLOv8 uses Soft-NMS which is a variant of the\\nNMS technique used in YOLOv5. Soft-NMS applies a soft\\nthreshold to the overlapping bounding boxes instead of dis-\\ncarding them outright. Whereas NMS removes the over-\\nlapping bounding boxes and keeps only the ones with the\\nhighest objectiveness score.\\nOutput heads refer to the ﬁnal layers of a neural network\\nthat predict the locations and classes of objects in an im-\\nage. In YOLO architecture there are typically several output\\nheads that are responsible for predicting different aspects of\\nthe detected objects, such as the bounding box coordinates,\\nclass probabilities, and objectiveness scores. These output\\nheads are typically connected to the last few layers of the\\nneural network and are trained to output a set of values that\\ncan be used to localize and classify objects in an image. The\\nnumber and type of output heads used can vary depending\\non the speciﬁc object detection algorithm and the require-\\nments of the task at hand. YOLOv5 has 3 output heads\\nwhile YOLOv8 has 1 output head. YOLOv8 Does not have\\nsmall, medium, and large anchor boxes rather it uses an\\nanchor free detection mechanism that directly predicts the\\ncenter of an object instead of the offset from a known an-\\nchor box which reduces the number of box predictions, and\\nthat speeds up the post processing process.\\nIt is fair to note that YOLOv8 is slightly slower than\\nYOLOv5 in regards to object detection speed. However,\\nYOLOv8 is still able to process images in real-time on mod-\\nern GPUs.\\nBoth YOLOv5 and YOLOv8 use mosaic augmentation\\non the training set. Mosaic augmentation is a data aug-\\nmentation technique that takes four random images from the\\ntraining set and combines them into a single mosaic image.\\nThis image, where each quadrant contains a random crop\\nfrom one of the four input images, is then used as input for\\nthe model [7].\\n9References\\n[1] AhmedMohsen. drone-detection-new dataset. https:\\n//universe.roboflow.com/ahmedmohsen/\\ndrone-detection-new-peksv , apr 2022. visited on\\n2023-05-03. 2, 6\\n[2] Hilary Beaumont. Virtual wall: how the us plans to boost\\nsurveillance at the southern border, 2023. April 3 2023. 1\\n[3] Aydin 1 Burchan and Singha Subroto. Yolov5, 2020. 7\\n[4] Aydin 1 Burchan and Singha Subroto. Drone detection using\\nyolov5. Eng 2023 , 4(1), 2023. 1, 7\\n[5] U.S. Customs and Border Protection. Human smugglers now\\nusing drones to surveil usbp, 2023. March 1 2023. 1\\n[6] John Davis. A watchful eye, 2022. Jan 4 2022. 1\\n[7] Brad Dwyer. Advanced augmentations in roboﬂow, 2020.\\n04-30-2023. 9\\n[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nSpatial pyramid pooling in deep convolutional networks for\\nvisual recognition. IEEE Transactions on Pattern Analysis\\nand Machine Intelligence , 37, 06 2014. 7\\n[9] Jan Hosang, Rodrigo Benenson, and Bernt Schiele. Learning\\nnon-maximum suppression. page 2, 05 2017. 7\\n[10] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu,\\nJun Li, Jinhui Tang, and Jian Yang. Generalized focal loss:\\nLearning qualiﬁed and distributed bounding boxes for dense\\nobject detection. CoRR , abs/2006.04388, 2020. 3\\n[11] new-workspace 0k81p. ﬂying object dataset\\ndataset. https://universe.roboflow.com/\\nnew-workspace-0k81p/flying_object_\\ndataset , mar 2022. visited on 2023-05-03. 2, 5\\n[12] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\\nFarhadi. You only look once: Uniﬁed, real-time ob-\\nject detection, 2016. Supplied as additional material\\nhttps://arxiv.org/pdf/1506.02640.pdf . 7\\n[13] Joseph Redmon and Ali Farhadi. Yolov3: An incremental\\nimprovement. CoRR , abs/1804.02767, 2018. 4\\n[14] Dillon Reis, Jacqueline Hong, Jordan Kupec, and Ahmad\\nDaoudi. Real time ﬂying object detection code repository.\\n1\\n[15] Zion Market Research. Global drone market size to register\\ncagr of about 38.75 percent over 2023-2030, 2023. March\\n15 2023. 1\\n[16] Jacob Solawetz and Francesco. What is yolov8? the ultimate\\nguide., 2023. 04-30-2023. 1, 5, 8\\n[17] Emma Soteriou. Ukraine ’tried to assassinate putin using\\ndrone loaded with explosives’ but it crashed miles from tar-\\nget, 2023. 27 April 2023. 1\\n[18] Juan R. Treven and Diana M. Cordova-Esparaza. A\\ncomprehensive review of yolo: From yolov1 to yolov8\\nand beyond, 2023. Supplied as additional material\\nhttps://arxiv.org/pdf/2304.00501.pdf . 7, 8\\n[19] Eastern District of California U.S. Attorney’s Ofﬁce. Four\\nindicted in scheme to deliver drugs into state prisons by\\ndrone, 2023. April 13 2023. 1\\n[20] Chien-Yao Wang, Hong-Yuan Mark Liao, I-Hau Yeh, Yueh-\\nHua Wu, Ping-Yang Chen, and Jun-Wei Hsieh. Cspnet: A\\nnew backbone that can enhance learning capability of CNN.\\nCoRR , abs/1911.11929, 2019. 7[21] Signate Yamaguchi. Mmyolo visualization, 2022. 3, 4\\n[22] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rong-\\nguang Ye, and Dongwei Ren. Distance-iou loss: Faster\\nand better learning for bounding box regression. CoRR ,\\nabs/1911.08287, 2019. 3\\n10'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cassio.init(token=ASTRA_DB_APPLICATION_TOKEN, database_id=ASTRA_DB_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astra_vector_store = Cassandra(\n",
    "    embedding=embedding,\n",
    "    table_name=\"demo\",\n",
    "    session=None,\n",
    "    keyspace=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astra_vector_store.add_texts(texts[:50])\n",
    "\n",
    "print(f\"Inserted {len(texts[:50])} headlines.\")\n",
    "\n",
    "astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_question = True\n",
    "while True:\n",
    "    if first_question:\n",
    "        query_text = input(\"\\nEnter your question (or type 'quit' to exit): \").strip()\n",
    "    else:\n",
    "        query_text = input(\"\\nWhat's your next question (or type 'quit' to exit): \").strip()\n",
    "\n",
    "    if query_text.lower() == \"quit\":\n",
    "        break\n",
    "\n",
    "    if query_text == \"\":\n",
    "        continue\n",
    "\n",
    "    first_question = False\n",
    "\n",
    "    print(\"\\nQUESTION: \\\"%s\\\"\" % query_text)\n",
    "    answer = astra_vector_index.query(query_text, llm=llm).strip()\n",
    "    print(\"ANSWER: \\\"%s\\\"\\n\" % answer)\n",
    "\n",
    "    print(\"FIRST DOCUMENTS BY RELEVANCE:\")\n",
    "    for doc, score in astra_vector_store.similarity_search_with_score(query_text, k=4):\n",
    "        print(\"    [%0.4f] \\\"%s ...\\\"\" % (score, doc.page_content[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScienceML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
